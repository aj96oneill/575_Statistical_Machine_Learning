{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import scipy.io\n",
    "import math\n",
    "import geneNewData\n",
    "\n",
    "def main():\n",
    "    myID='9485'\n",
    "    geneNewData.geneData(myID)\n",
    "    Numpyfile0 = scipy.io.loadmat('digit0_stu_train'+myID+'.mat')\n",
    "    Numpyfile1 = scipy.io.loadmat('digit1_stu_train'+myID+'.mat')\n",
    "    Numpyfile2 = scipy.io.loadmat('digit0_testset'+'.mat')\n",
    "    Numpyfile3 = scipy.io.loadmat('digit1_testset'+'.mat')\n",
    "    train0 = Numpyfile0.get('target_img')\n",
    "    train1 = Numpyfile1.get('target_img')\n",
    "    test0 = Numpyfile2.get('target_img')\n",
    "    test1 = Numpyfile3.get('target_img')\n",
    "    print([len(train0),len(train1),len(test0),len(test1)])\n",
    "    print('Your trainset and testset are generated successfully!')\n",
    "    pass\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullID='1209359485'\n",
    "name=\"Alex O'Neill\"\n",
    "myID='9485'\n",
    "Numpyfile0 = scipy.io.loadmat('digit0_stu_train'+myID+'.mat')\n",
    "Numpyfile1 = scipy.io.loadmat('digit1_stu_train'+myID+'.mat')\n",
    "Numpyfile2 = scipy.io.loadmat('digit0_testset'+'.mat')\n",
    "Numpyfile3 = scipy.io.loadmat('digit1_testset'+'.mat')\n",
    "train0 = Numpyfile0.get('target_img')\n",
    "train1 = Numpyfile1.get('target_img')\n",
    "test0 = Numpyfile2.get('target_img')\n",
    "test1 = Numpyfile3.get('target_img')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train0[0]))\n",
    "print(len(train0[0][0]))\n",
    "\n",
    "#Images are 28 rows by 28 columns\n",
    "train0[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(train0[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(numpy.mean(train0[0]))\n",
    "print(numpy.std(train0[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summation, total = 0, 0\n",
    "\n",
    "for row in train0[0]:\n",
    "    for i in row:\n",
    "        summation += i\n",
    "        total += 1\n",
    "print(summation)\n",
    "print(total)\n",
    "print(summation / total)\n",
    "mean = summation / total\n",
    "print(\"******\")\n",
    "holder = 0\n",
    "for row in train0[0]:\n",
    "    for i in row:\n",
    "        holder += (i - mean)**2\n",
    "print(numpy.sqrt(holder/total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "confirming numpy.mean and numpy.std gives correct value, and that I used it correctly\n",
    "\n",
    "# Task 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training0_feat1 = []\n",
    "training0_feat2 = []\n",
    "\n",
    "for image in train0:\n",
    "    training0_feat1.append(numpy.mean(image))\n",
    "    training0_feat2.append(numpy.std(image))\n",
    "print(len(training0_feat1))\n",
    "print(len(training0_feat2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training1_feat1 = []\n",
    "training1_feat2 = []\n",
    "\n",
    "for image in train1:\n",
    "    training1_feat1.append(numpy.mean(image))\n",
    "    training1_feat2.append(numpy.std(image))\n",
    "print(len(training1_feat1))\n",
    "print(len(training1_feat2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean of feature1 for digit0\n",
    "meanf1d0 = numpy.mean(training0_feat1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variance of feature1 for digit0\n",
    "varf1d0 = numpy.var(training0_feat1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean of feature2 for digit0\n",
    "meanf2d0 = numpy.mean(training0_feat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variance of feature2 for digit0\n",
    "varf2d0 = numpy.var(training0_feat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean of feature1 for digit1\n",
    "meanf1d1 = numpy.mean(training1_feat1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variance of feature1 for digit1\n",
    "varf1d1 = numpy.var(training1_feat1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean of feature2 for digit1\n",
    "meanf2d1 = numpy.mean(training1_feat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variance of feature2 for digit1\n",
    "varf2d1 = numpy.var(training1_feat2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Digit 0 f1 mean and variance: 44.19438724489796, 116.62085360643975 (0's mean brightness)\n",
    "Digit 0 f2 mean and variance: 87.403728874113725, 102.50446899504018 (0's std dev)\n",
    "\n",
    "Digit 1 f1 mean and variance: 19.382413010204083, 31.250672175624675 (1's mean brightness)\n",
    "Digit 1 f2 mean and variance: 61.388303358035159, 81.836603782790021 (1's std dev)\n",
    "\n",
    "It makes sense that Digit 1's avg brightness and std dev are lower, and with smaller variance, because the 0's use more pixels to create a sort of circle. Almost on average uses double the amount of brightened pixels. Because the 1 is fairly a straight line, the std dev should also be lower than 0 because the 0's will have many different ellipse type shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NB Classifier\n",
    "#P(y=C | x1...xn) = (P(y=C) *i..n*(P(xi|y=c)))\n",
    "#P(x=v|Ck) = (1 / sqrt(2pivarK))e^(-(v-meanK)^2/(2varK))\n",
    "\n",
    "def prob(value, mean, var):\n",
    "    return (1 / numpy.sqrt(2*numpy.pi*var)) * numpy.exp(-numpy.square(value - mean)/(2*var))\n",
    "\n",
    "results = []\n",
    "for image in test0:\n",
    "    image_mean = numpy.mean(image)\n",
    "    image_std = numpy.std(image)\n",
    "    prob0 = 0.5\n",
    "    prob1 = 0.5\n",
    "    prob0 *= (prob(image_mean, meanf1d0, varf1d0) * prob(image_std, meanf2d0, varf2d0))\n",
    "    prob1 *= (prob(image_mean, meanf1d1, varf1d1) * prob(image_std, meanf2d1, varf2d1))\n",
    "    results.append([prob0,prob1])\n",
    "pred0 = [0,0]\n",
    "for test in results:\n",
    "    pred0[numpy.argmax(test)] += 1\n",
    "    \n",
    "print(pred0)\n",
    "\n",
    "\n",
    "results = []\n",
    "for image in test1:\n",
    "    image_mean = numpy.mean(image)\n",
    "    image_std = numpy.std(image)\n",
    "    prob0 = 0.5\n",
    "    prob1 = 0.5\n",
    "    prob0 *= (prob(image_mean, meanf1d0, varf1d0) * prob(image_std, meanf2d0, varf2d0))\n",
    "    prob1 *= (prob(image_mean, meanf1d1, varf1d1) * prob(image_std, meanf2d1, varf2d1))\n",
    "    results.append([prob0,prob1])\n",
    "pred1 = [0,0]\n",
    "for test in results:\n",
    "    pred1[numpy.argmax(test)] += 1\n",
    "    \n",
    "print(pred1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy\n",
    "# The # correct / total for that class\n",
    "print(f\"Test0's accuracy: {pred0[0] / numpy.sum(pred0)}\")\n",
    "print(f\"Test1's accuracy: {pred1[1] / numpy.sum(pred1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Mean_of_feature1_for_digit0, Variance_of_feature1_for_digit0,\n",
    "\n",
    "# Mean_of_feature2_for_digit0, Variance_of_feature2_for_digit0\n",
    "\n",
    "# Mean_of_feature1_for_digit1, Variance_of_feature1_for_digit1\n",
    "\n",
    "# Mean_of_feature2_for_digit1, Variance_of_feature2_for_digit1,\n",
    "\n",
    "# Accuracy_for_digit0testset, Accuracy_for_digit1testset] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[44.19438724489796, 116.62085360643975, \n",
    " 87.403728874113725, 102.50446899504018, \n",
    " 19.382413010204083, 31.250672175624675, \n",
    " 61.388303358035159, 81.836603782790021, \n",
    " 0.9173469387755102, 0.9233480176211454]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
